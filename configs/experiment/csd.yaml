# @package _global_
general:
  # General settings
  name: 'csd_dual'      # Warning: 'debug' and 'test' are reserved name that have a special behavior

  wandb: 'offline'             # online | offline | disabled
  gpus: 2                    # Multi-gpu is not implemented on this branch
  guidance_target: 'EdgeComCondMulti'
  regressor: True

  resume: null            # If resume, path to ckpt file from outputs directory in main directory
  test_only: null         # Use absolute path

  check_val_every_n_epochs: 5
  sample_every_val: 4
  val_check_interval: null
  samples_to_generate: 256       # We advise to set it to 2 x batch_size maximum
  samples_to_save: 20
  chains_to_save: 5
  log_every_steps: 50
  number_chain_steps: 50        # Number of frames in each gif

  # final_model_samples_to_generate: 10000
  # final_model_samples_to_save: 30
  # final_model_chains_to_save: 20
  final_model_samples_to_generate: 3
  final_model_samples_to_save: 2
  final_model_chains_to_save: 1

  evaluate_all_checkpoints: False

model:
  # Model settings
  type: 'discrete'
  transition: 'marginal'                          # uniform or marginal
  model: 'graph_pos_com'
  diffusion_steps: 500
  diffusion_noise_schedule: 'cosine'              # 'cosine', 'polynomial_2', 'sigmoid'
  n_layers: 12


  extra_features: 'all'        # 'all', 'cycles', 'eigenvalues' or null

  # Do not set hidden_mlp_E, dim_ffE too high, computing large tensors on the edges is costly
  # At the moment (03/08), y contains quite little information
  hidden_mlp_dims: {'X': 256, 'E': 128, 'y': 256}

  # The dimensions should satisfy dx % n_head == 0
  hidden_dims : {'dx': 256, 'de': 64, 'dy': 128, 'n_head': 8, 'dim_ffX': 256, 'dim_ffE': 128, 'dim_ffy': 256}

  lambda_train: [2,6,12] # [3.69, 14.14, 32.17]

  # EGNNSparseNetwork
  train: True
  num_convs: 6 # global 6 local 4(geom) 6(qm9)
  num_atom: 15  # with_h: 6, without_h: 5
  hidden_dim: 256
  soft_edge: True
  norm_coors: True
  mlp_act: 'relu'
  cutoff: 2
  num_diffusion_timesteps: 1000
  edge_encoder: 'mlp'
  smooth_conv: False
  beta_schedule: 'sigmoid'
  beta_start: 1.e-7
  beta_end: 2.e-3
  context: ['pharma_score', 'SA', 'QED', 'acute_tox']

train:
# Training settings
  n_epochs: 300
  batch_size: 64  # 256, 512, 128, 64
  lr: 2e-4
  clip_grad: 0.2          # float, null to disable
  save_model: True
  num_workers: 0
  ema_decay: 0           # 'Amount of EMA decay, 0 means off. A reasonable value  is 0.999.'
  progress_bar: false
  weight_decay: 1e-12
  optimizer: adam # adamw,nadamw,nadam => nadamw for large batches, see http://arxiv.org/abs/2102.06356 for the use of nesterov momentum with large batches
  seed: 0
